{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shahedmobydeen/SCFP-benchmark/blob/main/Data_and_Model_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import argparse\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    EvalPrediction\n",
        ")\n",
        "from sklearn.metrics import accuracy_score, precision_recall_f1_score\n",
        "\n",
        "# --- Placeholder for LLM API Calls ---\n",
        "def generate_correction_triplet(problem_text: str, api_key: str = None) -> dict:\n",
        "    \"\"\"\n",
        "    Generates a self-correction triplet (initial response, critique, final response)\n",
        "    for a given problem using an external LLM API.\n",
        "\n",
        "    *** THIS IS A PLACEHOLDER FUNCTION ***\n",
        "    You must replace the logic below with your actual API calls to a service like\n",
        "    OpenAI, Anthropic, Google AI, etc.\n",
        "\n",
        "    Args:\n",
        "        problem_text: The input problem or prompt.\n",
        "        api_key: Your API key for the LLM service.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the generated triplet.\n",
        "    \"\"\"\n",
        "    print(f\"Generating triplet for: '{problem_text[:50]}...'\")\n",
        "    # 1. First API Call: Generate initial response\n",
        "    # client = YourLLMClient(api_key=api_key)\n",
        "    # initial_response = client.generate(prompt=f\"Solve this: {problem_text}. Think step-by-step.\")\n",
        "    initial_response = \"This is a placeholder initial response, containing a simulated thought process and a likely incorrect answer.\"\n",
        "\n",
        "    # 2. Second API Call: Generate self-critique\n",
        "    # critique_prompt = f\"Problem: {problem_text}\\nYour Answer: {initial_response}\\n\\nPlease review your answer for errors. Provide a critique.\"\n",
        "    # self_critique = client.generate(prompt=critique_prompt)\n",
        "    self_critique = \"This is a placeholder critique. It points out a simulated flaw in the initial response.\"\n",
        "\n",
        "    # 3. Third API Call: Generate final response\n",
        "    # final_prompt = f\"Problem: {problem_text}\\nYour Answer: {initial_response}\\nYour Critique: {self_critique}\\n\\nProvide a final, corrected answer.\"\n",
        "    # final_response = client.generate(prompt=final_prompt)\n",
        "    final_response = \"This is a placeholder final answer, supposedly corrected based on the critique.\"\n",
        "\n",
        "    return {\n",
        "        \"initial_response\": initial_response,\n",
        "        \"self_critique\": self_critique,\n",
        "        \"final_response\": final_response,\n",
        "    }\n",
        "\n",
        "\n",
        "def run_data_generation(args):\n",
        "    \"\"\"\n",
        "    Runs the data generation pipeline using source problems.\n",
        "    \"\"\"\n",
        "    print(\"--- Starting Data Generation ---\")\n",
        "    try:\n",
        "        source_df = pd.read_csv(args.source_problems_path)\n",
        "        print(f\"Loaded {len(source_df)} source problems.\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Source problems file not found at {args.source_problems_path}\")\n",
        "        return\n",
        "\n",
        "    results = []\n",
        "    for index, row in source_df.iterrows():\n",
        "        problem_text = row['problem_text'] # Assuming this column exists\n",
        "        triplet = generate_correction_triplet(problem_text)\n",
        "        results.append({\n",
        "            \"problem_id\": row.get('problem_id', f\"problem_{index}\"),\n",
        "            \"source_dataset\": row.get('source_dataset', 'unknown'),\n",
        "            \"problem_text\": problem_text,\n",
        "            **triplet\n",
        "        })\n",
        "\n",
        "    output_df = pd.DataFrame(results)\n",
        "    output_df.to_csv(args.output_path, index=False)\n",
        "    print(f\"Successfully generated and saved {len(output_df)} data points to {args.output_path}\")\n",
        "\n",
        "\n",
        "def run_training(args):\n",
        "    \"\"\"\n",
        "    Runs the meta-model training and evaluation pipeline.\n",
        "    \"\"\"\n",
        "    print(\"--- Starting Meta-Model Training ---\")\n",
        "\n",
        "    # 1. Load and prepare data\n",
        "    try:\n",
        "        df = pd.read_csv(args.data_path)\n",
        "        print(f\"Loaded {len(df)} records from {args.data_path}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Dataset not found at {args.data_path}\")\n",
        "        return\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n",
        "\n",
        "    # Concatenate inputs\n",
        "    df['text'] = df['problem_text'] + \" [SEP] \" + df['initial_response'] + \" [SEP] \" + df['self_critique']\n",
        "    df = df.rename(columns={'is_correct': 'label'})\n",
        "\n",
        "    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "    train_encodings = tokenizer(train_df['text'].tolist(), truncation=True, padding=True)\n",
        "    val_encodings = tokenizer(val_df['text'].tolist(), truncation=True, padding=True)\n",
        "\n",
        "    class SCFPDataset(torch.utils.data.Dataset):\n",
        "        def __init__(self, encodings, labels):\n",
        "            self.encodings = encodings\n",
        "            self.labels = labels\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "            item['labels'] = torch.tensor(self.labels[idx])\n",
        "            return item\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.labels)\n",
        "\n",
        "    train_dataset = SCFPDataset(train_encodings, train_df['label'].tolist())\n",
        "    val_dataset = SCFPDataset(val_encodings, val_df['label'].tolist())\n",
        "\n",
        "    # 2. Define metrics\n",
        "    def compute_metrics(p: EvalPrediction):\n",
        "        preds = np.argmax(p.predictions, axis=1)\n",
        "        precision, recall, f1, _ = precision_recall_f1_score(p.label_ids, preds, average='binary')\n",
        "        acc = accuracy_score(p.label_ids, preds)\n",
        "        return {'accuracy': acc, 'f1': f1, 'precision': precision, 'recall': recall}\n",
        "\n",
        "    # 3. Configure Trainer\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(args.model_name, num_labels=2)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=args.output_dir,\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=8,\n",
        "        warmup_steps=500,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir='./logs',\n",
        "        logging_steps=10,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    # 4. Run training and evaluation\n",
        "    print(\"Starting training...\")\n",
        "    trainer.train()\n",
        "\n",
        "    print(\"Evaluating final model...\")\n",
        "    eval_results = trainer.evaluate()\n",
        "    print(\"Evaluation Results:\", eval_results)\n",
        "\n",
        "    trainer.save_model(f\"{args.output_dir}/best_model\")\n",
        "    print(f\"Best model saved to {args.output_dir}/best_model\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description=\"SCFP Benchmark Data Generation and Model Training Pipeline\")\n",
        "    parser.add_argument(\"--mode\", type=str, required=True, choices=['generate', 'train'], help=\"Pipeline mode to run.\")\n",
        "\n",
        "    # Args for both modes\n",
        "    parser.add_argument(\"--output_path\", type=str, default=\"output.csv\", help=\"Path to save generated data.\")\n",
        "\n",
        "    # Args for 'generate' mode\n",
        "    parser.add_argument(\"--source_problems_path\", type=str, help=\"Path to CSV with source problems for data generation.\")\n",
        "\n",
        "    # Args for 'train' mode\n",
        "    parser.add_argument(\"--data_path\", type=str, help=\"Path to the SCFP benchmark CSV file for training.\")\n",
        "    parser.add_argument(\"--model_name\", type=str, default=\"microsoft/deberta-v3-base\", help=\"Hugging Face model to fine-tune.\")\n",
        "    parser.add_argument(\"--output_dir\", type=str, default=\"./scfp_model\", help=\"Directory to save trained model.\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    if args.mode == 'generate':\n",
        "        if not args.source_problems_path:\n",
        "            parser.error(\"--source_problems_path is required for 'generate' mode.\")\n",
        "        run_data_generation(args)\n",
        "    elif args.mode == 'train':\n",
        "        if not args.data_path:\n",
        "            parser.error(\"--data_path is required for 'train' mode.\")\n",
        "        run_training(args)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example Usage:\n",
        "    #\n",
        "    # To generate data (after filling in the placeholder function):\n",
        "    # python pipeline.py --mode generate --source_problems_path ./my_problems.csv --output_path ./generated_scfp_data.csv\n",
        "    #\n",
        "    # To train the model:\n",
        "    # python pipeline.py --mode train --data_path ./benchmark/SCFP_v1.0.csv --output_dir ./my_trained_model\n",
        "    main()"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'precision_recall_f1_score' from 'sklearn.metrics' (/usr/local/lib/python3.12/dist-packages/sklearn/metrics/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2335768343.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mEvalPrediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m )\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision_recall_f1_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# --- Placeholder for LLM API Calls ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'precision_recall_f1_score' from 'sklearn.metrics' (/usr/local/lib/python3.12/dist-packages/sklearn/metrics/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "Iwr6LDX1ryiV",
        "outputId": "9117f2fc-3089-41e3-d7ae-ae087a0c5460"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}